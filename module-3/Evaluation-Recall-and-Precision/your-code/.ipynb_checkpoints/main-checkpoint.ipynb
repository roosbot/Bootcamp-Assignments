{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Precision & Recall\n",
    "## Using the evaluation metrics we have learned, we are going to compare how well some different types of classifiers perform on different evaluation metrics\n",
    "### We are going to use a dataset of written numbers which we can import from sklearn. Run the code below to do so. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.datasets import fetch_mldata\n",
    "# mnist = fetch_mldata('MNIST original')\n",
    "# X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "# Solution found on https://stackoverflow.com/questions/54365045/scikit-learn-cannot-load-mnist-original-dataset-using-fetch-openml-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now take a look at the shapes of the X and y matricies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (70000, 784)\n",
      "y.shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's pick one entry and see what number is written. Use indexing to pick the 36000th digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   4., 149.,\n",
       "       255., 184.,  12.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  11., 133., 212., 253., 253., 253., 102.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 162., 236., 253., 253.,\n",
       "       253., 253., 253.,  55.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        35., 196., 253., 253., 253., 253., 253., 253., 239.,  18.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  89., 249., 253., 253., 253., 185.,\n",
       "       253., 253., 177.,  24.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 129.,\n",
       "       247., 253., 253., 165., 150., 205., 253., 139.,   3.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  89., 247., 253., 240., 131.,  85., 221.,\n",
       "       253., 253.,  84.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   4., 187.,\n",
       "       253., 253., 236., 139., 252., 253., 253., 253.,  84.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  21., 253., 253., 253., 253., 253., 253.,\n",
       "       253., 253., 248.,  53.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  99.,\n",
       "       253., 253., 253., 253., 253., 214., 253., 253., 179.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   4., 186., 251., 253., 249., 172.,\n",
       "       133., 253., 253., 137.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  49.,  94.,   6.,   0., 212., 253., 253.,  39.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       126., 253., 253., 197.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  27., 234., 253., 253.,  94.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       100., 253., 253., 239.,  11.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  61., 249., 253., 253.,  79.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   5.,\n",
       "       109., 253., 253., 193.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  66., 253., 253., 253.,  30.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       147., 253., 253., 182.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  99., 248., 253., 222.,  13.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[36000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use the .reshape(28,28) function and plt.imshow() function with the parameters cmap = matplotlib.cm.binary, interpolation=\"nearest\" to make a plot of the number. Be sure to import matplotlib!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use indexing to see if what the plot shows matches with the outcome of the 36000th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   4., 149., 255., 184.,  12.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,  11., 133., 212., 253., 253., 253., 102.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0., 162., 236., 253., 253., 253., 253., 253.,  55.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  35., 196., 253., 253., 253., 253., 253., 253., 239.,  18.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         89., 249., 253., 253., 253., 185., 253., 253., 177.,  24.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 129.,\n",
       "        247., 253., 253., 165., 150., 205., 253., 139.,   3.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  89., 247.,\n",
       "        253., 240., 131.,  85., 221., 253., 253.,  84.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   4., 187., 253.,\n",
       "        253., 236., 139., 252., 253., 253., 253.,  84.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  21., 253., 253.,\n",
       "        253., 253., 253., 253., 253., 253., 248.,  53.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  99., 253., 253.,\n",
       "        253., 253., 253., 214., 253., 253., 179.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   4., 186., 251.,\n",
       "        253., 249., 172., 133., 253., 253., 137.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49.,\n",
       "         94.,   6.,   0., 212., 253., 253.,  39.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0., 126., 253., 253., 197.,   6.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  27., 234., 253., 253.,  94.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0., 100., 253., 253., 239.,  11.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         61., 249., 253., 253.,  79.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   5.,\n",
       "        109., 253., 253., 193.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  66.,\n",
       "        253., 253., 253.,  30.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 147.,\n",
       "        253., 253., 182.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  99., 248.,\n",
       "        253., 222.,  13.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = X[36000,:].reshape(28,28)\n",
    "selection\n",
    "# plt.imshow(selection, cmap = \"binary\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets break into a test train split to run a classification. Instead of using sklearn, use indexing to select the first 60000 entries for the training, and the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:60000]\n",
    "X_test = X[60000:]\n",
    "y_train = y[:60000]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to make a two-class classifier, so lets restrict to just one number, for example 5s. Do this by defining a new y training and y testing sets for just the number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = np.where(y_train ==\"5\",True, False)\n",
    "y_test_5 = np.where(y_test ==\"5\",True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (60000, 784)\n",
      "y.shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape:\", X_train.shape)\n",
    "print(\"y.shape:\", y_train_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lets train a logistic regression to predict if a number is a 5 or not (remember to use the 'just 5s' y training set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=4000)\n",
    "model = model.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the classifier predict correctly the 36000th digit we picked before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[36000].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_5 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To make some comparisons, we are going to make a very dumb classifier, that never predicts 5s. Build the classifier with the code below, and call it using: never_5_clf = Never5Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "never_5_clf = Never5Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets fit and predict on the testing set using our never 5 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_never_5 = never_5_clf.predict(y_test_5)\n",
    "y_pred_never_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare this to the Logistic Regression. Examine the confusion matrix, precision, recall, and f1_scores for each. What is the probability cutoff you are using to decide the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9025   83]\n",
      " [ 145  747]]\n",
      "\n",
      "Precision y_pred: 0.9 \n",
      "Recall y_pred: 0.8374439461883408 \n",
      "F1_score y_pred: 0.867595818815331\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for y_pred\n",
    "matrix = confusion_matrix(y_test_5, y_pred_5)\n",
    "print(matrix)\n",
    "\n",
    "# Precision recall for y_pred\n",
    "precision_5 = precision_score(y_test_5, y_pred_5)\n",
    "recall_5 = recall_score(y_test_5, y_pred_5)\n",
    "f1_score_5 = f1_score(y_test_5, y_pred_5)\n",
    "\n",
    "print(\"\\nPrecision y_pred:\",precision_5,\"\\nRecall y_pred:\",recall_5,\"\\nF1_score y_pred:\",f1_score_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9108    0]\n",
      " [ 892    0]]\n",
      "\n",
      "Precision y_pred: 0.0 \n",
      "Recall y_pred: 0.0 \n",
      "F1_score y_pred: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for y_pred_never_5\n",
    "matrix = confusion_matrix(y_test_5, y_pred_never_5)\n",
    "print(matrix)\n",
    "\n",
    "# Precision recall for y_pred_never_5\n",
    "precision_never_5 = precision_score(y_test_5, y_pred_never_5)\n",
    "recall_never_5 = recall_score(y_test_5, y_pred_never_5)\n",
    "f1_score_never_5 = f1_score(y_test_5, y_pred_never_5)\n",
    "\n",
    "print(\"\\nPrecision y_pred:\",precision_never_5,\"\\nRecall y_pred:\",recall_never_5,\"\\nF1_score y_pred:\",f1_score_never_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the differences you see? Without knowing what each model is, what can these metrics tell you about how well each works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe first model had a higher precision score, recall and f1 score so I would choose the \\nfirst model. The second model always predicts True Negatives and False Negatives \\nbut never False Positives or True Positives.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The first model had a higher precision score, recall and f1 score so I would choose the \n",
    "first model. The second model always predicts True Negatives and False Negatives \n",
    "but never False Positives or True Positives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's examine the roc_curve for each. Use the roc_curve method from sklearn.metrics to help plot the curve for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8ac2d3ef10>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhU5fnG8e+TnS1hJ+wggogrGsG1oILiBl2soqWtltZuuGvVal3butVqXarFX621raVabQm7glDUCoKisigVUSGyC4Q1ITPz/v44kxCSQAaYmTNn5v5cVy4zmUnyHBJuX57znueYcw4REQm+LL8LEBGR+FCgi4ikCQW6iEiaUKCLiKQJBbqISJrI8esbt23b1vXo0cOvby8iEkjvvPPOBudcu4ae8y3Qe/Towfz58/369iIigWRmn+/tObVcRETShAJdRCRNKNBFRNKEAl1EJE0o0EVE0kSjgW5mz5jZOjNbtJfnzcweNbNlZvaBmR0X/zJFRKQxsazQnwWG7eP5c4De0bcrgCcPviwREdlfje5Dd87NNrMe+3jJCOA5583hnWNmLc2so3NudZxqFBEJJOcc5TurWF1ewZotFWzYuImeix6n2ak/5PDDj4z794vHhUWdgZW1HpdFP1Yv0M3sCrxVPN26dYvDtxYR8UcoHGHd1krWbKlgbXkFq8srWLvFC+6a98srqAxFADgpazH35TxN96x1zF3YFVI00K2BjzV41wzn3FhgLEBJSYnurCEiKWl7ZYg10UBeE11dV/+3OqjXb6uk7v2B8rKz6FCUT3FhAUd3aclZ/fLp0rSK0z9/jG6fvUhVUU+qhk9kYK/TElJ3PAK9DOha63EXYFUcvq6ISFxFIo4vt++qCeXV0dV1dVCvLvceb60M1fvcoia5FBcW0KGogL7FLSguakJxYQHFRfl0KCygY1ETWjXNxazWGvejyTDpOti2Fk65mtzBt0Buk4QdXzwCvRQYY2bjgIFAufrnIpJslaEw67ZU7m551FlZrymvYN3WCqrCey6rswzat/CC+tB2zTn10LZ0iAZ1cWETiosKKC4soEleduzFbFsPU34Gi1+G9kfAyOehc+I3ADYa6Gb2d2Aw0NbMyoA7gFwA59xTwGTgXGAZsAO4PFHFikjmcc6xpSK0u+UR7VfXbn+s2VLBxu276n1uk9xsOhYV0KGwgAE9W0dX0gXRwPbeb9s8n+yshjrHB1QsLHwRptwEu7bB6bfBKVdDTl58vn4jYtnlckkjzzvgp3GrSEQyRjjiWB89sbimfGc0nCuj7Y+drN1SyZryCnZWhet9bptmeTXBfGy3ll77I/q4OBrahQU5e7ZAEqm8DCZeBx9Pgy4nwPDHoX3f5HzvKN/G54pIetuxK1TnRGKlF9pboqEdbYFE6pxYzM022rfwVs/9OhVyRt/2ewR1cWEB7Qvzyc/ZjxZIIkUi8M6f4NU7wIVh2H0w4ArISn59CnQR2S/OOTZu31XvRGJ1UFevtLdU1D+x2KIgpyace7dvu7v9USuwWzfNIyteLZBE+/ITKL0SPn8TDhkMF/wOWvXwrRwFuojU2BWKsG7rnicS127Zc4/12vJKdoUje3yeGbRrnk9xUQE92jTjxEPa1AR1xyLvhGNxYQHN8tMkcsIheOtxmHUvZOd77ZX+o7w/CB+lyZ+uiOyLc46tlaE9d37U3a63pYIN2+qfWMzPyapZSR/XrdXu1XR0C1/HogLaNc8nJztDZv2tWQjjx8Dq96Dv+XDub6Cwo99VAQp0kcALRxxfbqusd4XiHhfDlFewfVf9E4utmubWnFg8uktRvfZHcWEBRU1yk3diMZWFKmH2g/DGw9CkFXzzWej3Vd9X5bUp0EVSWEVVuN4VirXbH97e6krCdc4s5mQZ7Vt4LZC+xS0Y1KddvX51h8ICCnJT5MRiqlv5trcq37AUjrkEzv41NG3td1X1KNBFfOCcY/OOquiJxN0tkLr96s07qup9bvP8HDoUemF9cq+20QtgCmquVuxQlE/bZvnBObGYynZthxn3wNynoKgLfOsl6D3E76r2SoEuEmdV4Qjrt1buvf1RZ2hTNTNo0yyfjkUFdGnVlJIe1f3qPS8xb1GQ69ORZZhPZsKEq2DzCjjhBzDkDshv4XdV+6RAF9kP2ypD9VfSdQJ7Q0NDm3Kyai58ObpLS84+onb7I5/ioia0b5FPbqacWExlOzfBK7fBgr9Cm0Ph8inQ/WS/q4qJAl2E3UObasK5gZGojQ1tKi4qoF/HwpoterUvMa83tElS04cTYNL1sH0DnHotDLoZcgv8ripmCnRJexVVu4c27b7EvLLOicX6Q5uyoycWOxTuHtpUs12v1ta9/RraJKlp2zqYfCMs+TcUHwWXvgCdjvW7qv2mQJfAcs6xZWeoXlDXvB+dCdLQ0Kamedk1wTywZ+uaVXX18KbieA9tktTkHLw/DqbeDFU74IxfeMO0soN5nkKBLikpFI6wYduu6ICmhmZXV7K6fCcVVZF6n9umWR7FRQV0Kiqgf/XQpqI991e3yE/i0CZJTZtXwsRrYNl06DrQu9qzXR+/qzooCnRJutpDm2pf+FL78fqtlQ0Obao+kdivUyFn9m1fs5+6ul+dUkObJDVFIjD/jzD9Tm+Ffs6DcML3ISv4J6QV6BI31UOb6p5IrDu7urGhTX06tKgX1IEb2iSpacPH3jCtFW9BrzPg/EegVXe/q4obBbrEZFcowtote16hWHeVvW5L/aFNWQbtWngXvlQPbappf9RqgTTN06+iJFC4Cv77GMy6z7sF3Fef9K74TLO2m/4WZbjaQ5tW12p/1L3XYkNDmwpys2pOJJZ0b9Xgdr2MGtokqWn1+95l+2s+gMOHe8O0WnTwu6qEUKCnsXDEsWFbZf2penVW1jv2MrTJu0Ixn6O7FEXvrZi/+9ZdhU0obKITi5LCqipg9gPwxiPQtA1c9Bz0G+F3VQmlQA+o2kOb9ja7em9DmzoUFtChMJ/DOxYy+LD2e9y1vPpuMBraJIG2Yo63Kv/yYzh2FJx1T0oO04o3BXqKqR7aVHeiXt1VdvnOhoc2Vfene1UPbaqeA1JYoKFNkv4qt8KMu+Htp6GoK4x6GQ490++qkkaBnkRV4QjrtlY2PAukVmA3NLSpbXPvxGKXVk05oUfrml0gtU8sNk+Xu8GIHIhl02HCNd7Nmgf+0LtIKL+531UllRIgTqqHNu25kt7zEvN9Dm0qKuDYri0b3K6noU0i+7BjI0y7Fd5/Htr2ge9NhW4n+l2VLxTojYhEHBu2V7J2j0vKK2qC2ruSsZJtDQxtatk0t2YXSL+OhXvcBaY6tFtqaJPIgVsyHibdADu+hNNugK/cGKhhWvGW0YFePbRpdTSk10aDes2WndG2iBfaocjehzb16dCC03q32/PS8mhga2iTSIJsXQOTb/CmI3Y8Bka9BB2P9rsq36VloFcPbVpdE8x1Z1dXsqZ8J5sauBtM07zsmlCuHtpU+9ZdHYsKaKOhTSL+cA7eex6m3eJtSxxyJ5x0JWSnZZTtt0D/KazcuIPpH65tcHZ1Q0Ob2jbPo0OhN7TpuOjQpurArn5fQ5tEUtSmz2HC1bB8JnQ7GYY/Bm0P9buqlBLoQL9vykdMWriavOws2hd6t+46snMRQw7vUK9f3aGwgLwcnVgUCZxI2NuGOONub8vXub+BktFpMUwr3gId6Dt2hTiiUyETrzxVq2qRdLR+qTdMa+VcOHSIN0yrZVe/q0pZgQ70UMSRl5OlMBdJN+EqePMR+M8DkNcMvvYHOPritBumFW+BDvRwxJGjk5Mi6WXVAhh/JaxdCEd8Dc55AJq397uqQAh0oIciTrtNRNJF1U5vvO1/H4Nm7eDiv8Hh5/tdVaDEdFbBzIaZ2VIzW2ZmNzfwfDczm2lmC8zsAzM7N/6l1hcKR8jRiRGR4PvsTXjyFK/Ncuyl8NO5CvMD0OgK3cyygSeAoUAZMM/MSp1zS2q97DbgBefck2bWD5gM9EhAvXsIRxw52VqhiwRWxRaYcRfM+z9o2R2+Mx4OGex3VYEVS8tlALDMObccwMzGASOA2oHugMLo+0XAqngWuTch9dBFguvjV71hWlu+gBN/Amfc5p0AlQMWS6B3BlbWelwGDKzzmjuBV8zsSqAZMKShL2RmVwBXAHTr1m1/a60nFFYPXSRwdmyEqbfAB+OgXV8Y/Sp0PcHvqtJCLA3ohhKzzsxALgGedc51Ac4F/mJm9b62c26sc67EOVfSrl27/a+2jlAkotubiQSFc7DoZXj8BFj0Txh0E/xwtsI8jmJZoZcBtXfyd6F+S2U0MAzAOfeWmRUAbYF18Shyb7RtUSQgtqyGSdfD0knQqT8MHw/FR/pdVdqJJdDnAb3NrCfwBTASuLTOa1YAZwLPmtnhQAGwPp6FNkTbFkVSnHOw4C8w7TYIV8LQe7x+uYZpJUSjf6rOuZCZjQGmAdnAM865xWZ2NzDfOVcKXA88bWbX4rVjLnOu7q0c4i8U1gpdJGVt/BQmXAWfzobup8LwR6FNL7+rSmsx/W/SOTcZbyti7Y/dXuv9JcAp8S2tcaGIUw9dJNVEwjD3D/DaPWDZcP7DcNxlGqaVBIH+d084EtEKXSSVrPsQxo+BL+ZD77O9MC/q7HdVGSPQga5tiyIpIrQL3ngYZj8I+S3g6/8HR12oYVpJFuxAjzjdPFnEb1+84w3TWrcYjrwQzrkfmrX1u6qMFOhAD2uXi4h/du2AWb+Gt56A5sVwyTg47By/q8pogQ70kHroIv749HVvB8vG5XD8ZTD0bigo8ruqjBfYQI9EHBGHVugiyVRRDq/eAe/8CVr1hO9OgJ5f8bsqiQpsoIci3jZ39dBFkmTpVJh4LWxbAyeNgdNvhbymflcltQQ20MPRQNcKXSTBtm+AKTd581fa94OL/wpdjve7KmlAYAO9KhIBUA9dJFGcg0UvwZSfeXPLB/8cTr0WcvL8rkz2IrCBHg57K3QFukgClH8Bk66D/02FzsfD8MehQz+/q5JGBDbQq3vo2eqhi8RPJALv/hlevR3CVXD2r2HgjyAr2+/KJAaBDfTqHrpW6CJx8uUnMOFq+Ox16HGaN0yr9SF+VyX7IbCBXhX2eug6KSpykMIhmPskvPYryM6FCx6F476jy/YDKLCBHq7ZtqhfOpEDtnaxN0xr1btw2Llw3kNQ2MnvquQABTbQa3roGskpsv9ClfD6Q95bQUu48Bk44utalQdcgANd2xZFDkjZfG9Vvv5DOPpiOPteaNbG76okDoIb6Nq2KLJ/dm33+uRzfu+1VS59Afqc7XdVEkeBDfSaXS7qoYs0bvl/vGFamz6DktEw5E4oKPS5KIm3wAa6eugiMdi5GV79Bbz7HLTuBZdNgh6n+l2VJEhwAz2sHrrIPn00CSZeB9vXwSlXw+BbILeJ31VJAgU20HVhkchebFvvzV9Z/DJ0OBIu+Tt0Ps7vqiQJAhvoIfXQRfbkHHzwAky9yTsBevptcOo13sVCkhECHOjVV4qqhy5CeZk3q/zjV6DLCd4wrfZ9/a5Kkiy4ga5tiyLeMK13noFX7wQXhmH3wYArNEwrQwU20LVtUTLehmVQeiWs+C8cMhgu+B206uFzUeKnwAZ6SCdFJVOFQ/DW4zDrXsjJhxFPwLHf0mX7EuRAVw9dMtCahTD+p7D6feh7vjdMq0Wx31VJighuoKuHLpkkVAmzH4Q3HoYmreCbf4Z+I7Qqlz0ENtDVQ5eMsWKu1yvfsBSOucS7i1DT1n5XJSkosIFeVXPpvwJd0lTlNnjtHpj7ByjqAt96CXoP8bsqSWExNaDNbJiZLTWzZWZ2815ec5GZLTGzxWb2fHzLrC8cvfQ/Vz10SUefvAZPngRzn4IBP4CfvKUwl0Y1ukI3s2zgCWAoUAbMM7NS59ySWq/pDdwCnOKc22Rm7RNVcLXdN4nWCl3SyM5NMO02eO+v0KY3XD4Vup/kd1USELG0XAYAy5xzywHMbBwwAlhS6zU/AJ5wzm0CcM6ti3ehdWmWi6SdDyfApOth+wY49ToYdBPkFvhdlQRILIHeGVhZ63EZMLDOa/oAmNmbQDZwp3Nuat0vZGZXAFcAdOvW7UDqrRFSD13Sxda1MOVGWDIeio/ybjzR6Vi/q5IAiiXQG0pM18DX6Q0MBroAr5vZkc65zXt8knNjgbEAJSUldb/GfqnetqgeugSWc/D+32HqLVC1E868HU6+SsO05IDFEuhlQNdaj7sAqxp4zRznXBXwqZktxQv4eXGpsgHhSAQzyNIKXYJo8wqYcA18MgO6ngjDH4N2ffyuSgIuluXtPKC3mfU0szxgJFBa5zX/Bk4HMLO2eC2Y5fEstK6qiFP/XIInEoG5Y+GJE2HFHDjnQbh8isJc4qLRFbpzLmRmY4BpeP3xZ5xzi83sbmC+c640+txZZrYECAM3Oue+TGTh4YgjR+0WCZINH8P4MbByDvQ6Ey54BFoe3LkkkdpiurDIOTcZmFznY7fXet8B10XfkiIU1gpdAiJcBf99FGbd790C7qtPeld86rJ9ibPAXikajkS0B11S3+r3vWFaaxZ6s1fOeRBadPC7KklTgQ109dAlpVVVwH/ugzcfhaZt4KK/QL/hflclaS6wgR4Oq4cuKerzt6B0DHy5DI4dBWf/0puQKJJggQ30UMTpoiJJLZVbYfpdMO9p72Tnt/8Fvc7wuyrJIAEO9IhG50rqWDbd21deXgYDfwRn/ALym/tdlWSYAAe6euiSAnZshGk/9674bNsHvjcNutWdjCGSHIENdPXQxVfOebNXJt/gTUg87Qb4yo0apiW+Cmygq4cuvtm6xpuK+NFE6HgMjHoZOh7td1UiQQ509dAlyZyD9/7mtVhClTDkLjhpDGQH9q+RpJnA/iaG1UOXZNr0GUy4GpbPgm4ne8O02h7qd1UiewhsoIfUQ5dkiITh7adhxl1gWXDeQ3D890C/e5KCghvokYgCXRJr/VJvmFbZ23DoUDj/YWjZtfHPE/FJgAPdUZCrloskQLgK3ngEZj8Aec3ga2Ph6Is0TEtSXmADXT10SYhVC7xV+dpFcMTX4ZwHoHk7v6sSiUlgA70q7MhWy0XipWonzLoX/vsYNGsPI5+Hvuf5XZXIfglsoIcjEa3QJT4+exNKr4SNn8Bx34Gh90CTln5XJbLfAhvooYjTPnQ5OBVbYPqdMP+P0LI7fGc8HDLY56JEDlxgA109dDko/3sFJl4DW1bBiT+FM271ToCKBFhgAz2kHrociO1fwtSbYeEL0K4vjH4Vup7gd1UicRHcQI9EyFXLRWLlHCx+GSb/DCo2w6Cb4LTrISff78pE4iawgR7WcC6J1ZbVMOk6WDoZOvWHEaXQ4Qi/qxKJu8AGelVYPXRphHPw7nPwyi8gXAln/RIG/ljDtCRtBfY321uhq4cue7HxU5hwFXw6G7qfCsMfhTa9/K5KJKECG+jqoUuDImGY+xTMuAeycuD8R+C472qYlmSEwAa6euhSz9olUDoGvngHep/tDdMq6ux3VSJJE8hAd86phy67hXbBG7+F2b+BgkL4xh/hyG9omJZknEAGesR5/83J1j+jM94X73jDtNYtgaO+CcPug2Zt/a5KxBeBDPRQJAKglksm27UDZv4K5vwemhfDJePgsHP8rkrEV8EM9LC3RFfLJUN9OhtKr4JNn8Lxl8PQu6CgyO+qRHwXzECP9lzUcskwFeXw6u3wzrPQqid8dwL0/IrfVYmkjEAGejiiFXrGWToFJl4L29bCyVfC4J9DXlO/qxJJKTEtcc1smJktNbNlZnbzPl53oZk5MyuJX4n1qYeeQbZvgH+Ohr+PhCat4fvTvSs+FeYi9TS6QjezbOAJYChQBswzs1Ln3JI6r2sBXAXMTUShtamHngGcg4X/hCk/g8qt3or81GshJ8/vykRSViwr9AHAMufccufcLmAcMKKB190DPABUxLG+BoXVQ09v5V94K/KXvw+tD4EfvQ6Db1KYizQilkTsDKys9bgs+rEaZtYf6Oqcm7ivL2RmV5jZfDObv379+v0utlpIPfT0FInA/GfgiYGw/D9w9q9h9CvQ/nC/KxMJhFhOijaUmq7mSbMs4GHgssa+kHNuLDAWoKSkxDXy8r0KhdVDTztffuJtRfz8DW/nygWPQuueflclEiixBHoZ0LXW4y7AqlqPWwBHArPMu9S6GCg1s+HOufnxKrS26hW6hnOlgXDIuzho5q8gOx+GPwb9v63L9kUOQCyBPg/obWY9gS+AkcCl1U8658qBmmutzWwWcEOiwhx299A1Pjfg1izyhmmtWgCHnQfnPQSFHf2uSiSwGg1051zIzMYA04Bs4Bnn3GIzuxuY75wrTXSRdamHHnChSnj9Ie+toCVc+Cc44mtalYscpJguLHLOTQYm1/nY7Xt57eCDL2vf1EMPsJXzvFX5+o/g6Iu9YVpNW/tdlUhaCOSVorsv/VegB8au7fDaL2HOk1DYCS59Efqc5XdVImklkIG++9J/9dADYfksbwfL5s+hZDQMudObWy4icRXIQK9SyyUYdm6GV26DBX+B1r3gssnQ4xS/qxJJW4EM9LC2Laa+jybBxOtg+3o45RoYfDPkNvG7KpG0FshAD9VsW1Sgp5xt67z5K4v/BR2OgkvHQaf+flclkhECGejqoacg5+CDf8DUm70ToGfc5q3Ms3P9rkwkYwQy0NVDTzGbV3qzype9Cl0GwIjHod1hflclknECGejqoaeISATm/xGm3wkuAsPuhwE/gKxsvysTyUiBDHT10FPAhmVQeiWs+C8ccjpc8Ai06uF3VSIZLZiBHlYP3TfhELz1GMy8F3ILYMTv4dhLddm+SAoIZKCHo7eg05WiSbZmIYz/Kax+H/qe7w3TalHsd1UiEhXIQNdwriSrqoDZD8Kbj3j39bzoOejX0E2rRMRPgQz0sHroybNirjdMa8P/4JhL4exfaZiWSIoKZKBXqYeeeJXbYMbd8PZYKOoCo16CQ4f4XZWI7EMgA1099ARbNgMmXAPlK71tiGfeDvkt/K5KRBoRyECv2baonRXxtXMTTLsV3vsbtOkNl0+B7if5XZWIxCiYgR52ZBlkqYceP0tKYfINsH0DnHodDLrJ25YoIoERzECPOHKy1T+Pi61rvSD/sBSKj4JvvQgdj/G7KhE5AIEM9HAkoi2LB8s5eO95mPZzqNrp9clPvkrDtEQCLJCBHoo4bVk8GJs+h4nXwCevQdcTYfhj0K6P31WJyEEKZqCHnVboByISgXlPw/S7vEv1z/2Nd0s4bf8USQvBDHT10Pff+v95w7RWzoFeZ3rDtFp287sqEYmjQAa6euj7IVwFb/4O/nM/5DaFrz4Fx4zUMC2RNBTIQA+F1UOPyar3vMv21yz0Zq+c+xto3t7vqkQkQYIZ6BFHrloue1e101uRv/koNGsLF/0F+g33uyoRSbBABnpYu1z27vO3vFX5l8ug/yg465fQpJXfVYlIEgQy0EPqoddXudXbvTLvae9k57f/Db1O97sqEUmiYAa6euh7+vhVb5jWli9g4I/hjNsgv7nfVYlIkgUz0LVt0bNjI0y9BT4YB20Pg9GvQNcBflclIj4JZKCHIxl+YZFzsOTfMPlGb0LiV2703nLy/a5MRHwU0zLXzIaZ2VIzW2ZmNzfw/HVmtsTMPjCzGWbWPf6l7lYVjmRuy2XrGvjHKHjxMijsDFfM8losCnORjNfoCt3MsoEngKFAGTDPzEqdc0tqvWwBUOKc22FmPwYeAC5ORMHgrdDzczOs5eIcLPirN688XAlD74YTfwrZgfxHlogkQCxpMABY5pxbDmBm44ARQE2gO+dm1nr9HGBUPIusKxRxNM2k+SObPoMJV8PyWdD9FLjgUWh7qN9ViUiKiSXQOwMraz0uAwbu4/WjgSkNPWFmVwBXAHTrduBzRDKmhx4Je/f0nHE3WDac91s4/nIN0xKRBsUS6A0lp2vwhWajgBJgUEPPO+fGAmMBSkpKGvwasciIHvq6j7wLhMrmwaFDvWFaRV38rkpEUlgsgV4GdK31uAuwqu6LzGwIcCswyDlXGZ/yGhaOOHLT9QbRoV3w5iMw+0HIaw5ffxqO+qaGaYlIo2IJ9HlAbzPrCXwBjAQurf0CM+sP/AEY5pxbF/cq6/Au/U/DtsMX73ojbtcugiO/AcPuh+bt/K5KRAKi0UB3zoXMbAwwDcgGnnHOLTazu4H5zrlS4EGgOfCieSvJFc65hE2Dqkq3S/+rdsLMX8Nbj0PzDjDy79D3XL+rEpGAiWnPm3NuMjC5zsdur/X+kDjXtU/hdLpj0WdveKvyjcvhuO962xGbtPS7KhEJoEBuYvYu/Q94oFdsgel3wPxnoFUP+E4pHNLguWQRkZgEMtADPz73f9Ng4rWwdTWcNAZO/znkNfO7KhEJuEAGelU4Qk4QT4pu/xKm3gwLX4B2feGi56BLid9ViUiaCGSgB+7CIudg0Usw5Wdeq2XQzXDadZq/IiJxFchAD0Uc2UHpoW9ZBZOuh6WTodNxMOJx6HCE31WJSBoKbKCn/ArdOXj3z/DKLyBc5d0K7sSfQFa235WJSJoKXKA756ItlxTuoW9cDqVXwWevQ4/T4ILfQZteflclImkucIEejngjYFJyhR4Jw5wn4bVfQnYunP+It7c8lf/nIyJpI3CBHooGesr10Ncu8YZpffEO9BnmTUYs6ux3VSKSQQIb6CmzQg/tgjd+C7N/AwWF8I0/enNYNExLRJIscIEeDlcHegq0Mcre8Vbl65Z4ExGH3Q/N2vhdlYhkqMAFeigSAfD30v9dO2Dmr2DO76F5MVzyDzhsmH/1iIgQyECP9tD9arl8OtsbprXpM+/uQUPvgoIif2oREaklsIGem+yWS0W5t6f83T9Dq57w3YnQ87Tk1iAisg+BC/TqHnpSV+hLp3jDtLathZOvhME/h7ymyfv+IiIxCFygVyWzh759gzd/ZdFL0P4IGPk36Hx84r+viMgBCFygh5PRQ3cOFr4IU26Cyq1w+q1wyjWQk5e47ykicpACF+ihRG9bLC+DidfBx9Ogc4k3TKv94Yn5XiIicRS4QE/Ypf+RCLzzJ3j1DnBhOPteGPhDDdeIOHEAAAaHSURBVNMSkcAIXKBX99Djeun/l594w7Q+fwN6DvKGabXuGb+vLyKSBIEL9HA8ty2GQzDnCZj5a8jOh+GPQf9v67J9EQmkwAV6KF7bFtcs8i7bX7UADjsPznsICjvGoUIREX8EL9APdttiqNIbpPXGb6FJK/jms9Dvq1qVi0jgBTDQD2KFvvJtGD8GNiyFo0fCsHuhaes4Vygi4o/ABXr1laL71UPftR1m3ANzn4LCzvCtf0LvoQmqUETEH4EL9P1eoX8yEyZcBZtXwAnfhzPv8OaWi4ikmQAGeow99J2b4ZVbYcFfoXUvuGwy9DglCRWKiPgjcIEe04VFH06ESdfD9vVw6rUw6CbIbZKkCkVE/BG4QN/npf/b1sHkG2HJv6HDUXDpOOjUP8kVioj4I3iB3tCVos7B++Ng6s1QtQPO+AWccjVk5/pUpYhI8gUw0Ou0XDavhInXwLLp0GWAN0yr3WE+Vigi4o+Y9v6Z2TAzW2pmy8zs5gaezzezf0Sfn2tmPeJdaLWaHro5ePtp+P2J8PlbcM4D8L2pCnMRyViNrtDNLBt4AhgKlAHzzKzUObek1stGA5ucc4ea2UjgfuDiRBQcCjsOsVUU/eOrUDYHDjndG6bVqnsivp2ISGDEskIfACxzzi13zu0CxgEj6rxmBPDn6Pv/BM40S8y19L3KXmZK3i1kb/gQRvwevv0vhbmICLEFemdgZa3HZdGPNfga51wIKAfa1P1CZnaFmc03s/nr168/oIKbdDyMxS1OYteP5kD/b2kGi4hIVCwnRRtKTHcAr8E5NxYYC1BSUlLv+VgMGHQ+DDr/QD5VRCStxbJCLwO61nrcBVi1t9eYWQ5QBGyMR4EiIhKbWAJ9HtDbzHqaWR4wEiit85pS4LvR9y8EXnPOHdAKXEREDkyjLRfnXMjMxgDTgGzgGefcYjO7G5jvnCsF/gj8xcyW4a3MRyayaBERqS+mC4ucc5OByXU+dnut9yuAb8a3NBER2R9xuDGniIikAgW6iEiaUKCLiKQJBbqISJowv3YXmtl64PMD/PS2wIY4lhMEOubMoGPODAdzzN2dc+0aesK3QD8YZjbfOVfidx3JpGPODDrmzJCoY1bLRUQkTSjQRUTSRFADfazfBfhAx5wZdMyZISHHHMgeuoiI1BfUFbqIiNShQBcRSRMpHeipdHPqZInhmK8zsyVm9oGZzTCzwN9/r7FjrvW6C83MmVngt7jFcsxmdlH0Z73YzJ5Pdo3xFsPvdjczm2lmC6K/3+f6UWe8mNkzZrbOzBbt5Xkzs0ejfx4fmNlxB/1NnXMp+YY3qvcT4BAgD3gf6FfnNT8Bnoq+PxL4h991J+GYTweaRt//cSYcc/R1LYDZwBygxO+6k/Bz7g0sAFpFH7f3u+4kHPNY4MfR9/sBn/ld90Ee81eA44BFe3n+XGAK3h3fTgTmHuz3TOUVekrdnDpJGj1m59xM59yO6MM5eHeQCrJYfs4A9wAPABXJLC5BYjnmHwBPOOc2ATjn1iW5xniL5ZgdUBh9v4j6d0YLFOfcbPZ957YRwHPOMwdoaWYdD+Z7pnKgx+3m1AESyzHXNhrv//BB1ugxm1l/oKtzbmIyC0ugWH7OfYA+Zvammc0xs2FJqy4xYjnmO4FRZlaGd/+FK5NTmm/29+97o2K6wYVP4nZz6gCJ+XjMbBRQAgxKaEWJt89jNrMs4GHgsmQVlASx/Jxz8Noug/H+Ffa6mR3pnNuc4NoSJZZjvgR41jn3kJmdhHcXtCOdc5HEl+eLuOdXKq/QM/Hm1LEcM2Y2BLgVGO6cq0xSbYnS2DG3AI4EZpnZZ3i9xtKAnxiN9Xd7vHOuyjn3KbAUL+CDKpZjHg28AOCcewsowBtila5i+vu+P1I50DPx5tSNHnO0/fAHvDAPel8VGjlm51y5c66tc66Hc64H3nmD4c65+f6UGxex/G7/G+8EOGbWFq8FszypVcZXLMe8AjgTwMwOxwv09UmtMrlKge9Ed7ucCJQ751Yf1Ff0+0xwI2eJzwX+h3d2/Nbox+7G+wsN3g/8RWAZ8DZwiN81J+GYpwNrgfeib6V+15zoY67z2lkEfJdLjD9nA34LLAEWAiP9rjkJx9wPeBNvB8x7wFl+13yQx/t3YDVQhbcaHw38CPhRrZ/xE9E/j4Xx+L3Wpf8iImkilVsuIiKyHxToIiJpQoEuIpImFOgiImlCgS4ikiYU6CIiaUKBLiKSJv4f30nkH5vVL54AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# y_pred_proba_5 = model.predict_proba(y_pred_5)[::,1]\n",
    "# y_pred_proba_never_5 = model.predict_proba(y_pred_never_5)[::,1]\n",
    "\n",
    "# fpr, tpr, threshold = roc_curve(y_test_5, y_pred_proba_5)\n",
    "# fpr2, tpr2, threshold2 = roc_curve(y_test_5, y_pred_proba_never_5)\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test_5, y_pred_5)\n",
    "fpr2, tpr2, threshold2 = roc_curve(y_test_5, y_pred_never_5)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(fpr2,tpr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now find the roc_auc_score for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score_5: 0.9141655391899105 \n",
      "roc_auc_score_never_5: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "roc_auc_score_5 = metrics.roc_auc_score(y_test_5, y_pred_5)\n",
    "roc_auc_score_never_5 = metrics.roc_auc_score(y_test_5, y_pred_never_5)\n",
    "\n",
    "print(\"roc_auc_score_5:\",roc_auc_score_5,\"\\nroc_auc_score_never_5:\",roc_auc_score_never_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this metric tell you? Which classifier works better with this metric in mind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The higher the score, the better the model is able to distinguish between 2 classes. \n",
    "0.5 is the worst and basically menas that it is randomly guessing.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
