{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inbalanced Classes\n",
    "## In this lab, we are going to explore a case of imbalanced classes. \n",
    "\n",
    "\n",
    "Like we disussed in class, when we have noisy data, if we are not careful, we can end up fitting our model to the noise in the data and not the 'signal'-- the factors that actually determine the outcome. This is called overfitting, and results in good results in training, and in bad results when the model is applied to real data. Similarly, we could have a model that is too simplistic to accurately model the signal. This produces a model that doesnt work well (ever). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, download the data from: https://www.kaggle.com/ntnu-testimon/paysim1. Import the dataset and provide some discriptive statistics and plots. What do you think will be the important features in determining the outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "data = pd.read_csv(\"../fraud.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample\n",
    "data = data.sample(n = 10000) \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect shape\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dtypes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "plt.scatter(x=\"newbalanceDest\",y=\"newbalanceDest\",data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "plt.scatter(x=\"amount\",y=\"newbalanceDest\",data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What do you think will be the important features in determining the outcome?\n",
    "\n",
    "I think the amount and newbalanceOrig will play a role.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the distribution of the outcome? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here\n",
    "# data[\"isFlaggedFraud\"].value_counts().plot.bar()\n",
    "data[\"isFlaggedFraud\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset. How are you going to integrate the time variable? Do you think the step (integer) coding in which it is given is appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we rename the time variable\n",
    "# data[\"time\"] = pd.to_timedelta(data[\"step\"], unit='h')\n",
    "# data[\"time\"] = pd.to_datetime(data[\"step\"])\n",
    "# # data[\"time\"] = pd.to_datetime(data[\"time\"])\n",
    "\n",
    "data.sort_values(by=\"step\", ascending=True)\n",
    "\n",
    "# Resource: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_timedelta.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "data.drop([\"nameOrig\",\"nameDest\",\"oldbalanceDest\",\"oldbalanceOrg\"], axis=1, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "We drop nameOrigin and nameDest because they are unique values.\n",
    "We drop oldbalanceDest & Org because it is directly with newbalanceDest & Org\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also create dummy variable of variable \"type\"\n",
    "data[\"type\"] = pd.get_dummies(data[\"type\"])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a logisitc regression classifier and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split dataframe\n",
    "data_x = data[data.columns.difference([\"isFraud\"])]\n",
    "data_y = data[\"isFraud\"]\n",
    "\n",
    "# Split data in test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y)\n",
    "\n",
    "# Initiate & fit model\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr = lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Predicted response:\", y_pred, sep=\"\\n\")\n",
    "\n",
    "# Actual response\n",
    "print(\"Actual response:\",y_test, sep=\"\\n\")\n",
    "\n",
    "# Check the accuracy of the model prediction\n",
    "lr_score = lr.score(y_test,y_pred)\n",
    "print(\"Accuracy score lr:\",lr_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now pick a model of your choice and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree\n",
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initiate & fit model\n",
    "rfc = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(\"Predicted response:\", y_pred, sep=\"\\n\")\n",
    "\n",
    "# Actual response\n",
    "print(\"Actual response:\",y_test, sep=\"\\n\")\n",
    "\n",
    "# Check the accuracy of the model prediction\n",
    "rfc_score = rfc.score(y_test,y_pred)\n",
    "print(\"Accuracy score rfc:\",rfc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-Nearest Neighbors\n",
    "# Import libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initiate & fit model\n",
    "knc = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_pred = knc.predict(X_test)\n",
    "print(\"Predicted response:\", y_pred, sep=\"\\n\")\n",
    "\n",
    "# Actual response\n",
    "print(\"Actual response:\",y_test, sep=\"\\n\")\n",
    "\n",
    "# Check the accuracy of the model prediction\n",
    "knc_score = knc.score(y_test,y_pred)\n",
    "print(\"Accuracy score knc:\",knc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model worked better and how do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score lr:\",lr_score)\n",
    "print(\"Accuracy score rfc:\",rfc_score)\n",
    "print(\"Accuracy score knc:\",knc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "They all produced the approximately same accuracy score\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
